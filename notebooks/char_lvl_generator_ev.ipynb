{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('tf-2.3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "f9d4a0f882bb371d98a0f4edb244a18811be9edcac9d5a483731180876028645"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'training_checkpoints'\n",
    "SAVE_DIR = 'saved_model'\n",
    "METADATA_DIR = 'metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mappings(alphabet):\n",
    "\n",
    "  char2ind = {ch:ind for ind, ch in enumerate(alphabet)}\n",
    "  ind2char = {ind:char for ind, char in enumerate(alphabet)}\n",
    "\n",
    "  return alphabet, char2ind, ind2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(METADATA_DIR, 'alphabet.pkl'), 'rb') as f:\n",
    "    al, char2ind, ind2char = make_mappings(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in char2ind.keys():\n",
    "    assert ind2char[char2ind[ch]] == ch\n",
    "\n",
    "for ind in ind2char.keys():\n",
    "    assert char2ind[ind2char[ind]] == ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_UNITS = 1024\n",
    "EMBEDDING_DIM = 256\n",
    "VOCAB_SIZE = len(char2ind.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'training_checkpoints/ckpt_100'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
    "\n",
    "pred_model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
    "\n",
    "pred_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (1, None, 256)            19968     \n_________________________________________________________________\ngru (GRU)                    (1, None, 1024)           3938304   \n_________________________________________________________________\ndense (Dense)                (1, None, 78)             79950     \n=================================================================\nTotal params: 4,038,222\nTrainable params: 4,038,222\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pred_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2ind[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # We pass the predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    text_generated.append(ind2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proviamo a l'ore un loco vanto.\n  Vedea colui che fe' di ggemo e 'l piano già scendevas veduto\nsi soleba colui che bene in peggior si ste della viverza.\n  Poi, rompendo del tomio l'alta mira?\n  El cominciò: «Sí volesse, m'è piú moviensi;\ne il figlio fu da fiorentin della luna\ncui men disdegno di Santa Pietase non uscise;\n  ma Virgilio mi disse: «Che pu eravamo,\ne piú di dubbio nella memoria mi si gira.\n  Quali a veder Conte\nquand'io mi trasmutai ad altra cura,\n  conosceresi un divina non piú non chiede verdi sí come fiede.\n  Ben se' tu manto che tosto raper quelli,\ne con le ciglia ne minacciando dicea: «Quando\ntu se' ogno in etterno ricco, e dentro ad esso, in su l'erba nel ciel si correda\ndi due radici della milla mia persona\ncon la sua donna e con la spirava\nin pria de' suoi ben far filostro ingegno sottile?\no delli angeli, ch'ancor si mosse della pietra il vinta,\ne a templ che si creda\noltre e Dio veramente piú lieve che per lo 'nferno fronda,\nin nostra redenzion pur ard\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(pred_model, start_string=u\"Proviamo \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            19968     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 78)             79950     \n",
      "=================================================================\n",
      "Total params: 4,038,222\n",
      "Trainable params: 4,038,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(SAVE_DIR, custom_objects={'loss': loss})\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Proviamo a terra grita.\n  Tutti cantavan: «Benedicta tu ogni pilose in su piú varso l'occhio alto,\nch'avra a sole il primo clima;\n  sí vid'io ben piú di mille splendori\nli morti li tiene e di cen parve me con parole scemo,\n  cosí quella pacili a men per verdini\ndel freddo ingogni a fante e ne serra.\n  Indi recherà la tua parola,\nper apprezza di famigliardi si centa;\n  e sí come dicea per sé stessa,\nch'uscir del primo cerchio dal becco si diparte.\n  Li parversi dipirto in bene assalvarsi;\n  e credenti alla tua preghieri da Marte,\nor dalla Pila e Bonifazio\nrimase all'un de' cigli! on sorpitra, e io tal, che noi spazio\ndopo lucida, se Dio non me ne spiego.\n  Prima era scendere e ciascun che certesi,\npareva ancor lo destro si stende\na piè di quello un altro coro a divorio sguardo, far noi posson con una inita cruna.\n  Pier cominciò a far sua guerra,\nquesta, privando regno pensier ch'armove\ndall'anime contenta.\n  Per non soffridar: 'Maria, ora è fatto vano,\narriva'mi a l'andar\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(new_model, start_string=u\"Proviamo \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}