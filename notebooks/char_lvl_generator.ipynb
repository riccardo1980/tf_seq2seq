{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.3 64-bit ('tf-2.3')",
      "metadata": {
        "interpreter": {
          "hash": "1b26f30d277d637ecb2900689acb928d3c296f454d4739bdd08be0222136dcd6"
        }
      }
    },
    "colab": {
      "name": "char_lvl_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMEnjEpt_3s-"
      },
      "source": [
        "# Character level generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqvmmTPS_3tC"
      },
      "source": [
        "1. Take a continuous flow of characters\n",
        "2. split it in chunks of SEQ_LENGTH\n",
        "3. evaluate labels by moving sequence by 1 to right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvgkMoyJstnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03420c0b-9766-4deb-8353-be8147d45581"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtSoKym0_3tF"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieob-08nJGIl"
      },
      "source": [
        "# DIVINA COMMEDIA\n",
        "path_to_file = '/content/drive/My Drive/DATASETS/DANTE_DIVINA_COMMEDIA/la_divin.txt'\n",
        "encoding = 'latin-1'\n",
        "subfolder = 'seq2seq_divina'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToHqYzKelCvp"
      },
      "source": [
        "CHECKPOINT_DIR = os.path.join('/content/drive/My Drive/colab',subfolder,'training_checkpoints')\r\n",
        "SAVE_DIR = os.path.join('/content/drive/My Drive/colab',subfolder,'saved_model')\r\n",
        "METADATA_DIR = os.path.join('/content/drive/My Drive/colab',subfolder,'metadata')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr-8xC9b_3t_"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding=encoding)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8epBA_8g_3uM"
      },
      "source": [
        "# collect all chars\n",
        "import pickle\n",
        "\n",
        "alphabet = list(set(text))\n",
        "\n",
        "if not os.path.exists(METADATA_DIR):\n",
        "    os.makedirs(METADATA_DIR)\n",
        "    \n",
        "with open(os.path.join(METADATA_DIR, 'alphabet.pkl'), 'wb') as f:\n",
        "  pickle.dump(alphabet, f)\n",
        "# TODO: save alphabet with order (to recover char2ind and ind2char)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59tpegC6I4xU"
      },
      "source": [
        "def make_mappings(alphabet):\n",
        "\n",
        "  char2ind = {ch:ind for ind, ch in enumerate(alphabet)}\n",
        "  ind2char = {ind:char for ind, char in enumerate(alphabet)}\n",
        "\n",
        "  return alphabet, char2ind, ind2char"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWQhEdTALJMw"
      },
      "source": [
        "with open(os.path.join(METADATA_DIR, 'alphabet.pkl'), 'rb') as f:\n",
        "    al, char2ind, ind2char = make_mappings(pickle.load(f))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9LOhRId_3uT"
      },
      "source": [
        "for ch in char2ind.keys():\n",
        "    assert ind2char[char2ind[ch]] == ch\n",
        "\n",
        "for ind in ind2char.keys():\n",
        "    assert char2ind[ind2char[ind]] == ind\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiMAA06J_3uW"
      },
      "source": [
        "# map input to a list of integer symbols\n",
        "text_as_int = np.array([char2ind[c] for c in text])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "WHaVppQC_3ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db31c558-061a-49d5-b796-20d92d4fbcb4"
      },
      "source": [
        "# list mapping of first characters\n",
        "# TEST\n",
        "print('{')\n",
        "for char,_ in zip(char2ind, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2ind[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  'é' :   0,\n",
            "  '.' :   1,\n",
            "  '-' :   2,\n",
            "  'x' :   3,\n",
            "  'D' :   4,\n",
            "  's' :   5,\n",
            "  'Q' :   6,\n",
            "  '«' :   7,\n",
            "  'U' :   8,\n",
            "  '\\x85':   9,\n",
            "  'v' :  10,\n",
            "  'í' :  11,\n",
            "  'o' :  12,\n",
            "  'ú' :  13,\n",
            "  'G' :  14,\n",
            "  'Ï' :  15,\n",
            "  'ë' :  16,\n",
            "  'F' :  17,\n",
            "  'y' :  18,\n",
            "  'ò' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_rpAQXE_o5Q"
      },
      "source": [
        "# Create the dataset\n",
        "- each example is a sequence of SEQ_LENGTH + 1 elements "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "JWSOc3nt_3ud"
      },
      "source": [
        "SEQ_LENGTH = 100\n",
        "EXAMPLES_PER_EPOCH = len(text) // SEQ_LENGTH\n",
        "\n",
        "char_dset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dset.batch(SEQ_LENGTH+1, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "VCtZeXoJ_3ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36db1e67-b1c9-4333-dd91-6fedc301470f"
      },
      "source": [
        "# TEST\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join([ ind2char[ind] for ind in item.numpy() ])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Dante Alighieri\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLA DIVINA COMMEDIA\\r\\n\\r\\nINFERNO\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCANTO PRIMO\\r\\n\\r\\n  Nel me'\n",
            "'zzo del cammin di nostra vita\\r\\nmi ritrovai per una selva oscura\\r\\nché la diritta via era smarrita.\\r\\n  '\n",
            "'Ah quanto a dir qual era è cosa dura\\r\\nesta selva selvaggia e aspra e forte\\r\\nche nel pensier rinova la'\n",
            "\" paura!\\r\\n  Tant'è amara che poco è piú morte;\\r\\nma per trattar del ben ch'io vi trovai,\\r\\ndirò dell'alt\"\n",
            "\"re cose ch'i' v'ho scorte.\\r\\n  Io non so ben ridir com'io v'entrai,\\r\\ntant'era pieno di sonno a quel pu\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acypAOef_3uk"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  \"\"\"\n",
        "  From a common sequence, generate input and target\n",
        "\n",
        "  :return:\n",
        "    input_text: elements from start to end-1\n",
        "    target_text: elements from start+1 to end\n",
        "  \"\"\"\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1MjQn98AWAq"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "r5t4Xl3l_3um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c99282c-a814-442c-fa86-02347c27a9f3"
      },
      "source": [
        "# TEST\n",
        "for inputs, targets in dataset.take(5):\n",
        "    print('in: {}'.format(repr(''.join([ ind2char[ind] for ind in inputs.numpy() ]))))\n",
        "    print('tg: {}\\n'.format(repr(''.join([ ind2char[ind] for ind in targets.numpy() ]))))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in: 'Dante Alighieri\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLA DIVINA COMMEDIA\\r\\n\\r\\nINFERNO\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCANTO PRIMO\\r\\n\\r\\n  Nel m'\n",
            "tg: 'ante Alighieri\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLA DIVINA COMMEDIA\\r\\n\\r\\nINFERNO\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCANTO PRIMO\\r\\n\\r\\n  Nel me'\n",
            "\n",
            "in: 'zzo del cammin di nostra vita\\r\\nmi ritrovai per una selva oscura\\r\\nché la diritta via era smarrita.\\r\\n '\n",
            "tg: 'zo del cammin di nostra vita\\r\\nmi ritrovai per una selva oscura\\r\\nché la diritta via era smarrita.\\r\\n  '\n",
            "\n",
            "in: 'Ah quanto a dir qual era è cosa dura\\r\\nesta selva selvaggia e aspra e forte\\r\\nche nel pensier rinova l'\n",
            "tg: 'h quanto a dir qual era è cosa dura\\r\\nesta selva selvaggia e aspra e forte\\r\\nche nel pensier rinova la'\n",
            "\n",
            "in: \" paura!\\r\\n  Tant'è amara che poco è piú morte;\\r\\nma per trattar del ben ch'io vi trovai,\\r\\ndirò dell'al\"\n",
            "tg: \"paura!\\r\\n  Tant'è amara che poco è piú morte;\\r\\nma per trattar del ben ch'io vi trovai,\\r\\ndirò dell'alt\"\n",
            "\n",
            "in: \"re cose ch'i' v'ho scorte.\\r\\n  Io non so ben ridir com'io v'entrai,\\r\\ntant'era pieno di sonno a quel p\"\n",
            "tg: \"e cose ch'i' v'ho scorte.\\r\\n  Io non so ben ridir com'io v'entrai,\\r\\ntant'era pieno di sonno a quel pu\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfKjLpuA_3uq"
      },
      "source": [
        "# configuration\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "RNN_UNITS = 1024\n",
        "EMBEDDING_DIM = 256\n",
        "VOCAB_SIZE = len(alphabet)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81YgLGvVAnae"
      },
      "source": [
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQj3IKRn_3ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe9bf7d-9894-4b53-a3e5-27ab9419dc64"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vS-sd1__3uw"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    \n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1RX_zQq_3uy"
      },
      "source": [
        "model =  build_model(\n",
        "    vocab_size = VOCAB_SIZE,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    rnn_units=RNN_UNITS,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "UUHRJ_SD_3u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf14f83a-d50c-4abb-d038-a75d632c5f2f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           19968     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 78)            79950     \n",
            "=================================================================\n",
            "Total params: 4,038,222\n",
            "Trainable params: 4,038,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "QUOdHfRw_3u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2165f9b8-c2d9-4272-9337-dfc5a79e6d8a"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 78) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRfSe4Ln_3u8"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Q8xorVAQ_3u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a72ec6d-4c34-40ed-9479-bd89ffe4d859"
      },
      "source": [
        "print('in: {}'.format(repr(''.join([ ind2char[ind] for ind in sampled_indices ]))))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in: \"ümfFìÏ\\x85dìB'gèf!\\x85èÀíbë\\riSoUabm-ýLxàÀRH'EcGDO[pTeZàúE[vJNÀ(qL)ésUeVìòR.àéAVDEBttoúbFúqrd\\x85RsÏOzQýF(\\nhlü\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "9d-9U7Xl_3vB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b4a6dd-aa91-40b4-af9f-c0ee2321c505"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 78)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.356991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27mH2UZ2_3vD"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK8dUhL4_3vG"
      },
      "source": [
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "P7KbIizO_3vM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a0a3b5-7f2c-4a2c-d2c6-de558a5df48c"
      },
      "source": [
        "EPOCHS=100\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "85/85 [==============================] - 6s 56ms/step - loss: 3.5881\n",
            "Epoch 2/100\n",
            "85/85 [==============================] - 5s 56ms/step - loss: 2.1782\n",
            "Epoch 3/100\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.9672\n",
            "Epoch 4/100\n",
            "85/85 [==============================] - 5s 57ms/step - loss: 1.8380\n",
            "Epoch 5/100\n",
            "85/85 [==============================] - 6s 58ms/step - loss: 1.7413\n",
            "Epoch 6/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.6550\n",
            "Epoch 7/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.5816\n",
            "Epoch 8/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.5256\n",
            "Epoch 9/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.4742\n",
            "Epoch 10/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.4268\n",
            "Epoch 11/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.3852\n",
            "Epoch 12/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 1.3491\n",
            "Epoch 13/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.3144\n",
            "Epoch 14/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.2769\n",
            "Epoch 15/100\n",
            "85/85 [==============================] - 6s 59ms/step - loss: 1.2407\n",
            "Epoch 16/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.2047\n",
            "Epoch 17/100\n",
            "85/85 [==============================] - 6s 59ms/step - loss: 1.1666\n",
            "Epoch 18/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.1271\n",
            "Epoch 19/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.0826\n",
            "Epoch 20/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 1.0368\n",
            "Epoch 21/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.9897\n",
            "Epoch 22/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.9378\n",
            "Epoch 23/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.8884\n",
            "Epoch 24/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.8397\n",
            "Epoch 25/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.7879\n",
            "Epoch 26/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.7403\n",
            "Epoch 27/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.6893\n",
            "Epoch 28/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.6454\n",
            "Epoch 29/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.6055\n",
            "Epoch 30/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.5669\n",
            "Epoch 31/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.5336\n",
            "Epoch 32/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.5086\n",
            "Epoch 33/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.4817\n",
            "Epoch 34/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.4613\n",
            "Epoch 35/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.4385\n",
            "Epoch 36/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.4276\n",
            "Epoch 37/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.4124\n",
            "Epoch 38/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3991\n",
            "Epoch 39/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3943\n",
            "Epoch 40/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3811\n",
            "Epoch 41/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3768\n",
            "Epoch 42/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3669\n",
            "Epoch 43/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3640\n",
            "Epoch 44/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3599\n",
            "Epoch 45/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3566\n",
            "Epoch 46/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3491\n",
            "Epoch 47/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3487\n",
            "Epoch 48/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3447\n",
            "Epoch 49/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3435\n",
            "Epoch 50/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3403\n",
            "Epoch 51/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3376\n",
            "Epoch 52/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3354\n",
            "Epoch 53/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3351\n",
            "Epoch 54/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3336\n",
            "Epoch 55/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3294\n",
            "Epoch 56/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3265\n",
            "Epoch 57/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3279\n",
            "Epoch 58/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3241\n",
            "Epoch 59/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3248\n",
            "Epoch 60/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3260\n",
            "Epoch 61/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3265\n",
            "Epoch 62/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3243\n",
            "Epoch 63/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3231\n",
            "Epoch 64/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3230\n",
            "Epoch 65/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3243\n",
            "Epoch 66/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3223\n",
            "Epoch 67/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3218\n",
            "Epoch 68/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3216\n",
            "Epoch 69/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3240\n",
            "Epoch 70/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3240\n",
            "Epoch 71/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3248\n",
            "Epoch 72/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3197\n",
            "Epoch 73/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3154\n",
            "Epoch 74/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3163\n",
            "Epoch 75/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3128\n",
            "Epoch 76/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3178\n",
            "Epoch 77/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3173\n",
            "Epoch 78/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3147\n",
            "Epoch 79/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3168\n",
            "Epoch 80/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3165\n",
            "Epoch 81/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3187\n",
            "Epoch 82/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3189\n",
            "Epoch 83/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3203\n",
            "Epoch 84/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3253\n",
            "Epoch 85/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3228\n",
            "Epoch 86/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3244\n",
            "Epoch 87/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3231\n",
            "Epoch 88/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3227\n",
            "Epoch 89/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3224\n",
            "Epoch 90/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3218\n",
            "Epoch 91/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3236\n",
            "Epoch 92/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3210\n",
            "Epoch 93/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3197\n",
            "Epoch 94/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3220\n",
            "Epoch 95/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3235\n",
            "Epoch 96/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3249\n",
            "Epoch 97/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3251\n",
            "Epoch 98/100\n",
            "85/85 [==============================] - 6s 61ms/step - loss: 0.3224\n",
            "Epoch 99/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3231\n",
            "Epoch 100/100\n",
            "85/85 [==============================] - 6s 60ms/step - loss: 0.3241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZQhxvTd_3vQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "38f0f23a-038a-4dda-835b-5e0bdef84cd7"
      },
      "source": [
        "tf.train.latest_checkpoint(CHECKPOINT_DIR)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/colab/seq2seq_divina/training_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoLIyCE9_3vT"
      },
      "source": [
        "pred_model = build_model(len(alphabet), EMBEDDING_DIM, RNN_UNITS, batch_size=1)\n",
        "\n",
        "pred_model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
        "\n",
        "pred_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bcTfuBx0_3vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41c4285-aa7f-41c9-ce15-bade76a0849b"
      },
      "source": [
        "pred_model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            19968     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 78)             79950     \n",
            "=================================================================\n",
            "Total params: 4,038,222\n",
            "Trainable params: 4,038,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AI3JIY8_3vZ"
      },
      "source": [
        "def generate_text(model, \n",
        "                  start_string, \n",
        "                  temperature: float=1.0, \n",
        "                  num_generate: int=1000) -> str:\n",
        "\n",
        "  \"\"\"\n",
        "    Evaluation step (generating text using the learned model)\n",
        "    \n",
        "    :param model:\n",
        "      TensorFlow learned model\n",
        "\n",
        "    :param start_string:\n",
        "      Initial string to feed to model\n",
        "    \n",
        "    :param temperature:\n",
        "      Low temperatures results in more predictable text.\n",
        "      Higher temperatures results in more surprising text.\n",
        "     \n",
        "    :param num_generate:\n",
        "      number of characters to generate\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2ind[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(ind2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ydDutjlT_3ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9026ba6b-43f6-4ee0-95bd-c53e11262ffd"
      },
      "source": [
        "print(generate_text(pred_model, start_string=u\"Nel mezzo del cammin\", temperature=1.1))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin disio\r\n",
            "dell'etterno, ch'era o alla cura:\r\n",
            "quel primo Maria, di sé in odo, ove s'arresta,\r\n",
            "avendo li occhi fuor cherci';\r\n",
            "e io rimasi in via con esso i due\r\n",
            "fu stadinanzi alli Anabbian conta».\r\n",
            "  E come a colui che novità del duolo.\r\n",
            "  Antandr c'hanno Italia morta,\r\n",
            "sí come nuvoleggia,\r\n",
            "e Faronici, non è l'altro caso e fera piú piú doglia\r\n",
            "se, ralla veduta ui colora,\r\n",
            "guardando le genti gloria e 'l Batista;\r\n",
            "per che 'l mio viso in lei tutte men ch'io questo mond'io vivo,\r\n",
            "era la mia virtú t'è in piacere?\r\n",
            "L'acqua di Monsibil per piú petto.\r\n",
            "  E tosto si vedrà di quel ch'io m'accuso\r\n",
            "per escusarmi, e vedermi dir vero;\r\n",
            "ché in te' nostro prede l'ardana e 'l triforme ai famigliarsi all'Espetto fissa gravidale ed olezzetta e terra,\r\n",
            "e la voce ond'io parea bello\r\n",
            "avvelson rabbiando,\r\n",
            "per lo diletto diversi sol di sé piglio.\r\n",
            "  Questi si percotean non può discittoso lume si dice\r\n",
            "tua cogntullo spoglio,\r\n",
            "  e «Ondo tu che, ma non arte, e parlar tu hoverchia,\r\n",
            "sí che terra non potesse al suo Ce\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkC6EvKH_3vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e847f5-13dc-4a97-f0d5-8f6d920d4827"
      },
      "source": [
        "print(generate_text(pred_model, start_string=u\"Questi la caccera per ogni villa\", temperature=1.1))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questi la caccera per ogni villa,\r\n",
            "fin che l'attese cortese oppinïone e m'accosa;\r\n",
            "  ma poi ch'i' fui agorello, il nostri piedi:\r\n",
            "miservi picciolse ancor di là non sia fosse quindi si leva.\r\n",
            "  Fu colui che si si risponde,\r\n",
            "  restare, voi bettiman nudo alla vita ria,\r\n",
            "è Azzolator, che pur dar piú cara,\r\n",
            "e comandò che l'amasseso quel frutto\r\n",
            "che fa in nube il vero inver la cala;\r\n",
            "  e io vi giugne, «or li canti, e poi tra lia\r\n",
            "colui che muta\r\n",
            "per l'aere tra Titoria tue».\r\n",
            "  Ed elli a me: «Tu vero appresso io mi presta\r\n",
            "ch'ella mi fece intrare appresso «il nido\r\n",
            "a cui tanto possion dentro si duce:\r\n",
            "  e vidi uscirci del suo dolce aspetto;\r\n",
            "  ma perché piebbi si levò dalpretolato pone, felice, e Don è vivo,\r\n",
            "avvolti, quando crea si pote da riversata,\r\n",
            "  gridò: «Perché, se ben t'accostò al cerebre che porta?\r\n",
            "ché, termine mi fe' l'E' miei passi\r\n",
            "tocchendo chi si nascose».\r\n",
            "  Ed elli a me: «Vano pensier che non ci ada la prima corno,\r\n",
            "la poco di quinci e quindi la mira\r\n",
            "esperighi mi or sí due noi ad una,\r\n",
            "o per lo lcco\r\n",
            "pont\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DexE7xQfJmOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b07591-7aae-4877-a50c-0fdf96be779d"
      },
      "source": [
        "pred_model.save(SAVE_DIR)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/colab/seq2seq_divina/saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/colab/seq2seq_divina/saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auPuics2IavT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b122f0ea-83de-4fa3-f5e4-6ef89cd94e6e"
      },
      "source": [
        "new_model = tf.keras.models.load_model(SAVE_DIR, custom_objects={'loss': loss})"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSPOY6pWIgc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d336d0c6-0c04-4c0a-81d7-3c509ad0da14"
      },
      "source": [
        "print(generate_text(new_model, start_string=u\"Poi fui famiglia del buon re Tebaldo:\"))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Poi fui famiglia del buon re Tebaldo:\r\n",
            "pria col ciel col sol ti stringo».\r\n",
            "  Quali i follini due che 'l sonno si spedí...\r\n",
            "  O mostrò che l'arca li si partí di dolorosanti passai che 'l ciel non è stato;\r\n",
            "però nel vi lasciato al cielo,\r\n",
            "tra 'l suo fattor lo sangue splendori.\r\n",
            "  L'altra è quella che leggi vicini.\r\n",
            "  O mostrava cia del loco dov'io stava,\r\n",
            "per avversari due perdonanzi»;\r\n",
            "venimmo al punto dove si dibasta.\r\n",
            "  Per per disiderie incontro a sé udito,\r\n",
            "quei che la ripa, ch'era per la spigliarmi,\r\n",
            "per lo 'nferno la Danuga,\r\n",
            "  e visse, e rimaser lenti;\r\n",
            "per che al cantar di là, ma perché sono\r\n",
            "la possa in quanto vi trasplo,\r\n",
            "com' credesti,\r\n",
            "lasciala per non veggio in sí fatta on affetto duro scese.\r\n",
            "  La donna mia agriglieo;\r\n",
            "  ché, se so chi è piú notar m'appressai;\r\n",
            "e 'l fummo del ruscel di sopra,\r\n",
            "se un cielo a veder com'io veggio in su la roclittura,\r\n",
            "corse rosplende,\r\n",
            "  vid'io farsi quel segno, che di Silvo».\r\n",
            "  Noi mi prescrise e la natural vedesse,\r\n",
            "com'e' dissi, ch'al ciel diventa degno.\r\n",
            "  Ma qui la morta p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaNRgz9fRQ2w"
      },
      "source": [],
      "execution_count": 41,
      "outputs": []
    }
  ]
}